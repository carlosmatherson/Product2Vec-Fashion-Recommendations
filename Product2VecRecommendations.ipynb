{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 2. Load Modules and Data","metadata":{"_uuid":"a4d70211-88ba-4b8b-afb6-2d4e561b56d3","_cell_guid":"c1ad4a56-176e-40d4-aae8-fd1cfc9f9fd5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install umap-learn","metadata":{"_uuid":"2bc1bcc0-c48e-4e1f-b7c9-5f9149387ace","_cell_guid":"f251a43a-ba05-48ae-8e83-7453ee4231ca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport umap        \nimport random\nimport os\nimport pickle\nimport logging\n\n\nfrom datetime import datetime, timedelta\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import TSNE\nfrom PIL import Image","metadata":{"_uuid":"0e8e45c1-f769-4654-b564-340ccf252add","_cell_guid":"1fe69e59-20fe-44f2-9f84-27bf8167e510","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Configure logging and plitting styles.","metadata":{"_uuid":"b2f0b909-a0ed-40e4-8dcd-3c08c43145bc","_cell_guid":"66ff73e6-8b77-45b8-abbf-e56311a8eb3a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n# plt.style.use('seaborn-v0_8-<seaborn-whitegrid>')\n# sns.set_palette('viridis')\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.width', 1000)","metadata":{"_uuid":"74760d52-fc2d-4a34-b7d9-ddfd6d5edd63","_cell_guid":"5eef0e22-3b80-4c94-b847-e41556d90708","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load in data:","metadata":{"_uuid":"c7f4a9ad-8a20-4c90-9c0a-5aa811064e67","_cell_guid":"126629a4-4443-4815-919a-0c1a0693cc3a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"txs_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/transactions_train.csv'\ncxs_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/customers.csv'\naxs_path = '/kaggle/input/h-and-m-personalized-fashion-recommendations/articles.csv'\nimg_path ='/kaggle/input/h-and-m-personalized-fashion-recommendations/images'\n\ndata = {\n    'transactions': pd.read_csv(txs_path, dtype={'article_id': str},parse_dates=['t_dat']),\n    'articles': pd.read_csv(axs_path, dtype={'article_id': str}),\n    'customers': pd.read_csv(cxs_path)\n}","metadata":{"_uuid":"85f179e5-07e8-4a51-91ad-148ecd430f78","_cell_guid":"5727c070-5c4f-4b1a-b029-8754b0b04beb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transactions = data['transactions']\narticles = data['articles']\ncustomers = data['customers']\n\ncustomers.fillna(0, inplace=True)  # replace NaNs to -1\n\n\nprint(f\"Transactions shape: {transactions.shape}\")\nprint(f\"Articles shape: {articles.shape}\")\nprint(f\"Customers shape: {customers.shape}\")","metadata":{"_uuid":"ced5c54f-22e6-42f9-bd85-e6defa9bcd59","_cell_guid":"ea9c661c-2ec1-445b-8a4a-f5332039f117","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Exploration\n\n### 3.1 Cursory View\n\nExplore transactions data:","metadata":{"_uuid":"2ffd3ca9-b744-435a-b244-141518e35dbf","_cell_guid":"0ce8734c-21a1-42d7-811c-bd0b65039ea0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"Transactions dataset info:\")\nprint(f\"Shape: {transactions.shape}\")\nprint(f\"Columns: {transactions.columns.tolist()}\")\nprint(\"\\nSample transactions:\")\ntransactions.head()","metadata":{"_uuid":"2e4f6598-e2ec-4428-89b7-b5192555b0c9","_cell_guid":"2871b855-b9e8-4c35-9da0-82be600d7757","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Explore articles of clothing:","metadata":{"_uuid":"e3e49c1c-877e-4786-9390-504f183243b0","_cell_guid":"98dc9ea9-712d-481f-83b1-210ad88e6bab","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Explore articles data\nprint(\"Articles dataset info:\")\nprint(f\"Shape: {articles.shape}\")\nprint(f\"Columns: {articles.columns.tolist()}\")\nprint(\"\\nSample articles:\")\narticles.head()","metadata":{"_uuid":"99c88d61-53ab-48bc-828b-1ede9dad77f0","_cell_guid":"e39060cb-02ec-4216-9733-0076ebc9c994","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Explore customer data:","metadata":{"_uuid":"0a75a658-97a8-40cf-a5b0-fb21d77abbc1","_cell_guid":"4e8d6922-546b-4ab5-83ed-21ffed7183c7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Explore customers data\nprint(\"Customers dataset info:\")\nprint(f\"Shape: {customers.shape}\")\nprint(f\"Columns: {customers.columns.tolist()}\")\nprint(\"\\nSample customers:\")\ncustomers.head()","metadata":{"_uuid":"a36adaae-a261-41ef-bb58-7111208bf9c4","_cell_guid":"a3779ae4-14a3-471f-a1cd-e3a43c526816","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2 Transactions Analysis\n\nTake a look at purchases by day, week, and month. This is important to identify trends in customer behavior. This may be useful for providing recommendations relevant to to the season or current week.","metadata":{"_uuid":"cafbc7a7-45c4-4dd8-8407-a3d09af71d29","_cell_guid":"c69e7762-75b0-4cf3-a02d-9c7bd2f8f334","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(f\"Date range: {transactions['t_dat'].min()} to {transactions['t_dat'].max()}\")\n\ntransactions['week'] = transactions['t_dat'].dt.to_period('W')\ntransactions['month'] = transactions['t_dat'].dt.to_period('M')\n\ndaily_purchases = transactions.groupby('t_dat').size()\nweekly_purchases = transactions.groupby('week').size()\nmonthly_purchases = transactions.groupby('month').size()\n\n# Plot weekly and monthly trends\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 10))\n\ndaily_purchases.plot(ax=ax1)\nax1.set_title('Daily Purchase Volume')\nax1.set_ylabel('Number of Purchases')\nax1.set_xlabel('Day')\nax1.grid(True)\n\nweekly_purchases.plot(ax=ax2)\nax2.set_title('Weekly Purchase Volume')\nax2.set_ylabel('Number of Purchases')\nax2.set_xlabel('Week')\nax2.grid(True)\n\nmonthly_purchases.plot(ax=ax3)\nax3.set_title('Monthly Purchase Volume')\nax3.set_ylabel('Number of Purchases')\nax3.set_xlabel('Month')\nax3.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"794cb47f-1d72-40c8-9169-a295fb84045d","_cell_guid":"fc95411f-31f0-42e9-8c91-4bb4f39cba05","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3 Customer Analysis\n\nLook at customer purchase frequency on linear and logular scale so we can see the long tail of the distribution. Every non-outlier customer has fewer than 1250 purchases, which is evident on the log scale. There is a large frequency around zero purchases.","metadata":{"_uuid":"7267914d-a6f5-4b1c-9134-7ece56f4f965","_cell_guid":"f7172852-63de-4212-83a0-d0e480fe8d26","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Customer purchase frequency\ncustomer_purchase_counts = transactions.groupby('customer_id').size()\n\nprint(f\"Average purchases per customer: {customer_purchase_counts.mean()}\")\nprint(f\"Median purchases per customer: {customer_purchase_counts.median()}\")\nprint(f\"Min purchases per customer: {customer_purchase_counts.min()}\")\nprint(f\"Max purchases per customer: {customer_purchase_counts.max()}\")\n\n# lin distribution\nplt.figure(figsize=(12, 6))\ncustomer_purchase_counts.hist(bins=50)\nplt.title('Distribution of Purchases per Customer')\nplt.xlabel('Number of Purchases')\nplt.ylabel('Number of Customers')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# log distribution\nplt.figure(figsize=(12, 6))\ncustomer_purchase_counts.hist(bins=50, log=True)\nplt.title('Distribution of Purchases per Customer (Log Scale)')\nplt.xlabel('Number of Purchases')\nplt.ylabel('Number of Customers (Log Scale)')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"3595d410-e546-48b9-905e-a009bfd6339e","_cell_guid":"2bc9c525-480b-413a-89d1-ce3d5137d622","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.4 Product Analysis\n\nNow, we can take a look at product popularity by item and by category. The most popular item is a pair of Jade HW Skinny Denim TRS, and the most populr category is trousers.","metadata":{"_uuid":"45fc4ca2-74c6-4f65-b8bd-3b13696250d3","_cell_guid":"4adb12ee-caba-4167-a0ae-83b753de7a34","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"product_popularity = transactions['article_id'].value_counts()\n\nprint(f\"Total unique products purchased: {len(product_popularity)}\")\nprint(f\"Average purchases per product: {product_popularity.mean()}\")\nprint(f\"Median purchases per product: {product_popularity.median()}\")\nprint(f\"Min purchases per product: {product_popularity.min()}\")\nprint(f\"Max purchases per product: {product_popularity.max()}\")","metadata":{"_uuid":"1cf9fecc-15aa-49d4-9323-6960cdc3db64","_cell_guid":"f55a4342-5aad-4d9e-b124-309ae73a7ce4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# map article_id to product name\narticle_names = articles[['article_id', 'prod_name']].set_index('article_id')['prod_name']\n\n# replace article_ids with product names\nproduct_popularity_named = product_popularity.copy()\nproduct_popularity_named.index = [article_names.get(id, id) for id in product_popularity.index]\n\nmost_popular_article_id = product_popularity.index[0]\nmost_popular_article_name = product_popularity_named.index[0]\n\nplt.figure(figsize=(12, 6))\nproduct_popularity_named.head(50).plot(kind='bar')\nplt.title('Top 50 Most Popular Products')\nplt.xlabel('Product Name')\nplt.ylabel('Purchase Count')\nplt.grid(True, axis='y')\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"7a3e6099-e120-42cc-8b19-8ab490faa3a1","_cell_guid":"14b497ce-84cb-4502-bf21-72447cbc9563","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subfolder = most_popular_article_id[:3]\nimg_path = os.path.join(images_dir, subfolder, f\"{most_popular_article_id}.jpg\")\n\nplt.figure(figsize=(6, 6))\nimg = Image.open(img_path)\nplt.imshow(img)\nplt.title(f\"Most Popular Item: {most_popular_article_name}\", fontsize=14)\nplt.axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"1e807ce4-c126-4e61-a39c-cdd73792b0dd","_cell_guid":"83cae37f-ef6b-4bc3-91c3-1dec93aa16d4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transactions_with_products = transactions.merge(articles, on='article_id', how='left')\n\nproduct_type_counts = transactions_with_products['product_type_name'].value_counts()\nproduct_group_counts = transactions_with_products['product_group_name'].value_counts()\n\nplt.figure(figsize=(14, 8))\nproduct_type_counts.head(20).plot(kind='barh')\nplt.title('Top 20 Product Types')\nplt.xlabel('Purchase Count')\nplt.ylabel('Product Type')\nplt.grid(True, axis='x')\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"fad5ed7f-5791-4da3-ba86-cdcaef1ccb60","_cell_guid":"fb207904-82ff-4f45-aa98-38142443dc22","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Data Preprocessing","metadata":{"_uuid":"2236b2d4-6fb0-439b-8a70-0f4f8800ad1e","_cell_guid":"48dcded8-f191-4c6c-882e-77295a873da3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def prepare_transaction_sequences(transactions, min_purchases=2, max_seq_length=50):\n\n    print(\"Preparing transaction sequences...\")\n    \n    # sort transactions by customer and date\n    transactions_sorted = transactions.sort_values(['customer_id', 't_dat'])\n    \n    # group by customer_id and gather purchase sequences\n    customer_sequences = []\n    \n    for customer_id, group in tqdm(transactions_sorted.groupby('customer_id')):\n        if len(group) >= min_purchases:\n            # Get purchased articles sequence\n            sequence = group['article_id'].tolist()\n            \n            # limit sequence length\n            if len(sequence) > max_seq_length:\n                sequence = sequence[-max_seq_length:]\n                \n            customer_sequences.append(sequence)\n    \n    print(f\"Created {len(customer_sequences)} customer sequences\")\n    return customer_sequences","metadata":{"_uuid":"b377d0bf-0ff3-45a5-b963-48c552ad0d35","_cell_guid":"af5a58d3-58a9-4a37-b3f1-a26c6346125b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_time_splits(transactions, test_days=7, validation_days=7):\n\n    print(\"Creating time splits...\")\n    \n    # set split dates\n    max_date = transactions['t_dat'].max()\n    test_start_date = max_date - timedelta(days=test_days-1)\n    validation_start_date = test_start_date - timedelta(days=validation_days)\n    \n    # split data\n    train = transactions[transactions['t_dat'] < validation_start_date]\n    validation = transactions[(transactions['t_dat'] >= validation_start_date) & \n                              (transactions['t_dat'] < test_start_date)]\n    test = transactions[transactions['t_dat'] >= test_start_date]\n    \n    print(f\"Train period: until {validation_start_date - timedelta(days=1)}\")\n    print(f\"Validation period: {validation_start_date} to {test_start_date - timedelta(days=1)}\")\n    print(f\"Test period: {test_start_date} to {max_date}\")\n    \n    print(f\"Train shape: {train.shape}\")\n    print(f\"Validation shape: {validation.shape}\")\n    print(f\"Test shape: {test.shape}\")\n    \n    return {\n        'train': train,\n        'validation': validation,\n        'test': test\n    }","metadata":{"_uuid":"c053699c-5256-4c12-9393-0ab1f0477aef","_cell_guid":"40993283-3783-41fb-8a08-660053c31aa5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_ground_truth(transactions, customer_ids, start_date, days=7):\n\n    print(\"Creating ground truth...\")\n    \n    end_date = start_date + timedelta(days=days-1)\n    \n    # filter transactions for the evaluation period\n    eval_transactions = transactions[(transactions['t_dat'] >= start_date) & \n                                    (transactions['t_dat'] <= end_date)]\n    \n    # make dictionary\n    ground_truth = {}\n    \n    for customer_id, group in eval_transactions.groupby('customer_id'):\n        if customer_id in customer_ids:\n            # get unique purchased articles\n            purchased_articles = group['article_id'].unique().tolist()\n            ground_truth[customer_id] = purchased_articles\n    \n    print(f\"Created ground truth for {len(ground_truth)} customers\")\n    return ground_truth","metadata":{"_uuid":"1873217a-fdf9-4aa8-bf27-74b968cc992e","_cell_guid":"023b5e20-5afe-4dda-b92a-d11b98cc83e2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Product2Vec Model Implementation\n\nCreate a class for the Product2Vec model:","metadata":{"_uuid":"df4841e4-2543-4521-b5ae-6491659d3a13","_cell_guid":"487b7de8-164b-4d70-86db-8ab34e68a560","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Product2Vec model for learning product embeddings from purchase sequences using word2vec\nclass Product2Vec:\n\n    def __init__(self, vector_size=100, window=5, min_count=5, \n                 sg=1, workers=4, epochs=20, ns_exponent=0.75):\n\n        self.vector_size = vector_size\n        self.window = window\n        self.min_count = min_count\n        self.sg = sg\n        self.workers = workers\n        self.epochs = epochs\n        self.ns_exponent = ns_exponent\n        self.model = None\n        self.word_vectors = None\n        self.item_embeddings = None\n        self.item_biases = None\n        \n    def fit(self, sequences):\n\n        print(f\"Training Product2Vec with {len(sequences)} sequences...\")\n\n        # convert sequences dictionary to list if needed\n        if isinstance(sequences, dict):\n            sequences = list(sequences.values())\n        \n        # train\n        self.model = Word2Vec(\n            sentences=sequences,\n            vector_size=self.vector_size,\n            window=self.window,\n            min_count=self.min_count,\n            sg=self.sg,\n            workers=self.workers,\n            epochs=self.epochs,\n            ns_exponent=self.ns_exponent\n        )\n        \n        # store embeddings\n        self.item_embeddings = {item: self.model.wv[item] \n                                for item in self.model.wv.index_to_key}\n        \n        # store biases\n        if hasattr(self.model, 'trainables') and hasattr(self.model.trainables, 'biases'):\n            self.item_biases = {item: self.model.trainables.biases[self.model.wv.key_to_index[item]] \n                                for item in self.model.wv.index_to_key}\n        \n        print(f\"Model trained with {len(self.item_embeddings)} items\")\n        \n        return self\n    \n    def get_embedding(self, item_id):\n        return self.model.wv[item_id]\n    \n    def get_similar_items(self, item_id, top_n=12): # playing with top_n, was 10\n        return self.model.wv.most_similar(item_id, topn=top_n)\n\n    \n    def get_recommendations(self, history, top_n=12, strategy='mean', \n                           recency_bias=True, exclude_history=True):\n\n        # filter history items that are in the vocabulary\n        valid_history = [item for item in history if item in self.model.wv.key_to_index]\n        \n        if not valid_history:\n            # if no valid items in history, return empty list\n            return []\n        \n        # apply recency bias if enabled\n        if recency_bias and len(valid_history) > 1:\n            # more recent items get higher weights\n            weights = np.linspace(0.5, 1.0, len(valid_history))\n        else:\n            weights = np.ones(len(valid_history))\n        \n        # Get embeddings for history items\n        history_embeddings = [self.get_embedding(item) for item in valid_history]\n        \n        # combine embeddings based on strategy\n        if strategy == 'mean':\n            user_vector = np.mean(history_embeddings, axis=0)\n        elif strategy == 'weighted_mean':\n            user_vector = np.average(history_embeddings, axis=0, weights=weights)\n        elif strategy == 'recent':\n            # Use only the most recent item\n            user_vector = history_embeddings[-1]\n        else:\n            raise ValueError(f\"Unknown strategy: {strategy}\")\n        \n        # find similar items to the user vector\n        items_to_exclude = set(valid_history) if exclude_history else set()\n        similar_items = self._find_similar_items(user_vector, top_n, items_to_exclude)\n        \n        return [item_id for item_id, _ in similar_items]\n    \n    def _find_similar_items(self, vector, top_n=12, exclude_items=None):\n\n        if exclude_items is None:\n            exclude_items = set()\n        \n        # norm the query vector\n        norm_vector = vector / np.linalg.norm(vector)\n        \n        # get all item vectors\n        all_items = [(item, self.model.wv[item]) for item in self.model.wv.index_to_key \n                    if item not in exclude_items]\n        \n        # Calculate similarities\n        similarities = [(item, np.dot(norm_vector, vec / np.linalg.norm(vec))) \n                       for item, vec in all_items]\n        \n        # sort similarities descendingly \n        similarities.sort(key=lambda x: x[1], reverse=True)\n        \n        return similarities[:top_n]\n    \n    def save(self, model_path, embeddings_path=None):\n\n        # save the model\n        self.model.save(model_path)\n        \n        # save embeddings\n        if embeddings_path:\n            with open(embeddings_path, 'wb') as f:\n                pickle.dump(self.item_embeddings, f)\n    \n    @classmethod\n    def load(cls, model_path, embeddings_path=None):\n\n        # create a new instance\n        instance = cls()\n        \n        # load the Word2Vec model\n        instance.model = Word2Vec.load(model_path)\n        \n        # load embeddings if path provided, otherwise extract from model\n        if embeddings_path and os.path.exists(embeddings_path):\n            with open(embeddings_path, 'rb') as f:\n                instance.item_embeddings = pickle.load(f)\n        else:\n            instance.item_embeddings = {item: instance.model.wv[item] \n                                     for item in instance.model.wv.index_to_key}\n        \n        return instance","metadata":{"_uuid":"ce210116-3a4d-427c-8af3-277f65bc2c18","_cell_guid":"fb4b1fd1-8298-42ad-b0ca-a28f12e0b1a5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_user_recommendations(model, user_histories, top_n=12, \n                                strategy='weighted_mean', recency_bias=True):\n\n    print(f\"Generating recommendations for {len(user_histories)} users...\")\n    \n    recommendations = {}\n    \n    for user_id, history in tqdm(user_histories.items()):\n        recs = model.get_recommendations(\n            history=history,\n            top_n=top_n,\n            strategy=strategy,\n            recency_bias=recency_bias\n        )\n        \n        # if not enough recommendations, pad with popular items\n        if len(recs) < top_n:\n\n            # do not include already recommended items\n            remaining_popular = [item for item in popular_items if item not in recs]\n            \n            # add popular items until we reach fill up top_n slots\n            needed = top_n - len(recs)\n            recs.extend(remaining_popular[:needed])\n                \n        recommendations[user_id] = recs[:top_n]\n    \n    print(f\"Generated recommendations for {len(recommendations)} users\")\n    return recommendations","metadata":{"_uuid":"426dcf3e-46b1-4057-9f82-9745f22de663","_cell_guid":"9dbbe47c-13b2-4741-8d43-649ab17af5d3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_item_sequences(transactions, column='article_id', time_column='t_dat', \n                         customer_column='customer_id'):\n\n    # sort transactions by customer and time\n    sorted_df = transactions.sort_values([customer_column, time_column])\n    \n    # group by customer and create sequences\n    sequences = {}\n    \n    for customer, group in tqdm(sorted_df.groupby(customer_column)):\n        sequences[customer] = group[column].tolist()\n    \n    return sequences","metadata":{"_uuid":"e5dbfac8-d2ad-42c3-957a-d1e8e09149e9","_cell_guid":"abb2dee9-b5bf-4b52-9013-67280a7daef3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"8615450a-1ab1-4e73-a7bd-857679877fc5","_cell_guid":"1ecd71f0-4378-4aaa-b7dd-b41f67953fc1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_params = {\n    'vector_size': 100,\n    'window': 5,\n    'min_count': 5,\n    'sg': 1,  # Skip-gram\n    'workers': 4,\n    'epochs': 20,\n    'ns_exponent': 0.75  # Negative sampling distribution exponent\n}\n\ntrain_sequences = prepare_transaction_sequences(transactions, min_purchases=2)\ncustomer_histories = create_item_sequences(transactions)\n \n# initialize and train the model\nbase_model = Product2Vec(**base_params)\nbase_model.fit(train_sequences)","metadata":{"_uuid":"f04a117e-0410-4edf-a78b-5497a6deaa92","_cell_guid":"e3fca80b-0c74-47dd-be5b-efc37f70a768","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# find similar products for an example item\narticle_id = \"0110065011\"  \nsimilar_items = base_model.get_similar_items(article_id, top_n=5)\nprint(f\"Similar products to {article_id}:\")\nfor similar_id, similarity in similar_items:\n    print(f\"  {similar_id} (Similarity: {similarity})\")","metadata":{"_uuid":"c4aa13de-ec20-45cf-930b-aac37d2f1c3d","_cell_guid":"543dd2c0-df4c-431c-97a6-cf871e8b8184","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate recommendations for a user\ncustomer_id = \"00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2c5feb1ca5dff07c43e\"  \nhistory = transactions[transactions['customer_id'] == customer_id]['article_id'].tolist()\nrecommendations = base_model.get_recommendations(\n    history=history,\n    top_n=12,\n    strategy='weighted_mean',\n    recency_bias=True\n)\nprint(f\"Recommendations for customer {customer_id}:\")\nprint(recommendations)","metadata":{"_uuid":"e829f0ce-a9e7-451e-954f-0c6c34535a73","_cell_guid":"3819e779-7ba2-4453-b1e8-864507ffcd3a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_product_recommendations(article_ids, articles_df=None, title=\"Product Recommendations\"):\n\n\n    fig, axes = plt.subplots(3, 4, figsize=(15, 10))\n    fig.suptitle(title, fontsize=16)\n    \n    axes = axes.flatten()\n    \n    for i, article_id in enumerate(article_ids[:12]):\n        \n        subfolder = article_id[:3]\n        img_path = os.path.join(images_dir, subfolder, f\"{article_id}.jpg\")\n        \n        if os.path.exists(img_path):\n            img = Image.open(img_path)\n            axes[i].imshow(img)\n            axes[i].set_title(f\"Article: {article_id}\", fontsize=10)\n        else:\n            print('oopsie')\n        \n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.subplots_adjust(top=0.9)\n    plt.show()","metadata":{"_uuid":"45ba1728-7e44-4dbb-b860-6e1d866d7bfd","_cell_guid":"ade48a86-2a37-4b90-9178-b5b7d4ec21ae","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_recommendations_for_customer(customer_id, model, transactions, articles_df=None):\n\n    # get customer purchase history\n    history = transactions[transactions['customer_id'] == customer_id]['article_id'].tolist()\n    \n    # generaet recommendations\n    recommendations = model.get_recommendations(\n        history=history,\n        top_n=12,\n        strategy='weighted_mean',\n        recency_bias=True\n    )\n    \n    print(f\"Displaying {len(recommendations)} recommendations for customer {customer_id}\")\n    display_product_recommendations(recommendations, articles_df=articles_df, \n                                    title=f\"Recommendations for Customer {customer_id}\",\n                                    img_path)\n    \n    return recommendations","metadata":{"_uuid":"673c28eb-065f-4a6c-9233-feab56a76dbc","_cell_guid":"5be57ae5-d533-4f4a-aa3c-7f046d646cd0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Evaluation","metadata":{"_uuid":"f47e581c-a1cf-4a91-a111-6f9f62ace5fb","_cell_guid":"7ec9cd38-5c2b-4c37-8b4c-e10e3441aa84","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 5.1 Model Evaluations","metadata":{"_uuid":"5a10a532-7bf8-4ef0-acbb-b0aee257ba00","_cell_guid":"8ff620a5-2e28-4fc1-b8c9-27a7c3cff51c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def map_at_k_kaggle(actual, predicted, k=12):\n    \n    ap_at_k = []\n    \n    for act, pred in zip(actual, predicted):\n        if len(act) > 0:\n            pred_k = pred[:k]\n            \n            precision_sum = 0\n            num_correct = 0\n            \n            for i, item in enumerate(pred_k):\n                if item in act:\n                    num_correct += 1\n                    precision_sum += num_correct / (i + 1)\n            \n            ap = precision_sum / min(len(act), k) if len(act) > 0 else 0\n            ap_at_k.append(ap)\n    \n    return np.mean(ap_at_k) if ap_at_k else 0","metadata":{"_uuid":"71392198-9b55-4dce-a0df-abed7b6b5387","_cell_guid":"9943c38a-b384-4fba-ba9d-b4eb6f53ca5e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, user_histories, ground_truth, top_n=12, strategy='weighted_mean', \n                  recency_bias=True):\n\n    print(f\"Evaluating model with strategy '{strategy}'...\")\n    \n    recommendations = {}\n    actual_lists = []\n    pred_lists = []\n    \n    eval_users = list(set(user_histories.keys()) & set(ground_truth.keys()))\n    print(f\"Evaluating for {len(eval_users)} users\")\n    \n    for user_id in tqdm(eval_users):\n        history = user_histories[user_id]\n        actual = ground_truth[user_id]\n        \n        recs = model.get_recommendations(\n            history=history,\n            top_n=top_n,\n            strategy=strategy,\n            recency_bias=recency_bias\n        )\n        \n        if len(recs) > 0:\n            recommendations[user_id] = recs\n            actual_lists.append(actual)\n            pred_lists.append(recs)\n    \n    map_score = map_at_k_kaggle(actual_lists, pred_lists, k=top_n)\n    \n    all_items = set(item for user, items in ground_truth.items() for item in items)\n    recommended_items = set(item for user, items in recommendations.items() for item in items)\n    \n    item_coverage = len(recommended_items & all_items) / len(all_items) if len(all_items) > 0 else 0\n    customer_coverage = len(recommendations) / len(ground_truth) if len(ground_truth) > 0 else 0\n    \n    return {\n        'map@12': map_score,\n        'item_coverage': item_coverage,\n        'customer_coverage': customer_coverage,\n        'recommendations': recommendations\n    }","metadata":{"_uuid":"c563ada7-384c-4e47-a15d-1fdb887af8dc","_cell_guid":"8d1a8697-7ee3-4122-aeb6-d785f6a74de8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create time-based splits\nsplits = create_time_splits(transactions, test_days=7, validation_days=7)\ntrain_df = splits['train']\nvalidation_df = splits['validation']\ntest_df = splits['test']","metadata":{"_uuid":"5d5c68c8-11cb-4470-874b-215b607da87d","_cell_guid":"91fdf269-5415-4202-b79b-cfb8e843d657","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create ground truth\nvalidation_ground_truth = create_ground_truth(\n    validation_df, \n    list(customer_histories.keys()), \n    validation_df['t_dat'].min(),\n    days=7\n)","metadata":{"_uuid":"e6388dd5-4803-4b57-8e5a-1a7b8b7df770","_cell_guid":"28a383e1-03c2-461e-b626-1b02b89fcc1a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate model\nresults = evaluate_model(\n    base_model,\n    customer_histories,\n    validation_ground_truth,\n    top_n=12,\n    strategy='weighted_mean',\n    recency_bias=True\n)\n\nprint(f\"MAP@12: {results['map@12']}\")\nprint(f\"Item coverage: {results['item_coverage']}\")\nprint(f\"Customer coverage: {results['customer_coverage']}\")","metadata":{"_uuid":"51ae5a0b-9124-4e16-8812-e1b2426344a0","_cell_guid":"b5d86938-d6ac-4384-b442-6d72c32d2de7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2 Visualize Embeddings","metadata":{"_uuid":"e00d803f-0943-4ee6-b6de-ab031bae50ff","_cell_guid":"addeb27e-4f92-4f4d-a8ad-03b42813d6e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# 3. Define the missing visualization function\ndef visualize_embeddings(model, articles_df, dim_reduction='tsne', n_items=1000, \n                        color_by='product_group_name', figsize=(12, 10)):\n\n    # get embeddings for visualization\n    vocab_items = list(model.model.wv.key_to_index.keys())\n    \n    # dim reduction options\n    if dim_reduction == 'tsne':\n        reducer = TSNE(n_components=2, random_state=19)\n    elif dim_reduction == 'umap':\n        reducer = umap.UMAP(random_state=19)\n    \n    # sample items if there are too many\n    if len(vocab_items) > n_items:\n        sample_items = random.sample(vocab_items, n_items)\n    else:\n        sample_items = vocab_items\n    \n    item_embeddings = np.array([model.model.wv[item] for item in sample_items])\n    \n    reduced_embeddings = reducer.fit_transform(item_embeddings)\n    \n    # create df for plotting\n    plot_df = pd.DataFrame({\n        'x': reduced_embeddings[:, 0],\n        'y': reduced_embeddings[:, 1],\n        'article_id': sample_items\n    })\n    \n    plot_df = plot_df.merge(articles_df, on='article_id', how='left')\n    \n    plt.figure(figsize=figsize)\n    sns.scatterplot(data=plot_df, x='x', y='y', hue=color_by, alpha=0.7, s=20)\n    plt.title(f'Product Embeddings Visualization ({dim_reduction.upper()})')\n    plt.xlabel(f'{dim_reduction.upper()} Dimension 1')\n    plt.ylabel(f'{dim_reduction.upper()} Dimension 2')\n    \n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    \n    return plt.gcf()","metadata":{"_uuid":"cfd7f531-4806-45bf-a1db-081107405a39","_cell_guid":"569b7dd8-b903-440e-b2e8-6d2780209997","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualize embeddings using t-SNE:","metadata":{"_uuid":"dce435d4-1fe8-461d-84a1-d05ecd0dc411","_cell_guid":"bc3240f6-39c4-4299-af09-bec558ba1824","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"tsne_fig = visualize_embeddings(\n    base_model, \n    articles, \n    dim_reduction='tsne', \n    n_items=2000, \n    color_by='product_group_name',\n    figsize=(15, 12)\n)","metadata":{"_uuid":"c1b21530-8c06-48cd-b99d-ff6f10d22823","_cell_guid":"6c704f52-0ad5-482a-9564-7745384dea59","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualize embeddings using UMAP:","metadata":{"_uuid":"c4971736-c5d4-4e61-8b47-0c80aba02821","_cell_guid":"19aaf662-a0f9-4e5d-8b12-04b38821d84e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"","metadata":{"_uuid":"211fa1f3-336c-45eb-afa4-2834c06343dd","_cell_guid":"3cd18125-f3be-49a9-b845-b9018bb795c2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"umap_fig = visualize_embeddings(\n    base_model, \n    articles, \n    dim_reduction='umap', \n    n_items=2000, \n    color_by='colour_group_name',\n    figsize=(15, 12)\n)","metadata":{"_uuid":"645d8e31-6fa0-4315-a245-4fbae02ce6df","_cell_guid":"2c4bf1c8-2bc0-4ea4-8434-41850e929f4b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##","metadata":{"_uuid":"3f5dce02-d268-445a-9f9a-85fe709f3cbc","_cell_guid":"c4330132-3aac-4de1-b7f7-3abdb81bffa9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}